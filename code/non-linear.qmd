---
title: "Non-linear regression on AirBnb dataset"
author: "Xiaojing Ni Group 15"
format:
    html:
        embed-resources: true
        theme: default
        code-copy: true
        code-line-numbers: true
        number-sections: true
        toc: true
---

```{r results='hide', warning=FALSE, message=FALSE}
library(caret)
library(dplyr)
library(tidyr)
library(rpart)
library(rattle)
library(ggplot2)
library(stringr)
library(reshape2)
library(tidyverse)
library(visNetwork)



```

```{r}
## read data

airbnb <- read.csv("../data/Aemf1.csv")
str(airbnb)
```

```{r}
airbnb$City <- factor(airbnb$City)
airbnb$Day <- factor(airbnb$Day)
airbnb$Room.Type <- factor(airbnb$Room.Type)
airbnb$Shared.Room <- factor(airbnb$Shared.Room)
airbnb$Private.Room  <- factor(airbnb$Private.Room )
airbnb$Superhost  <- factor(airbnb$Superhost )
airbnb$Multiple.Rooms  <- factor(airbnb$Multiple.Rooms )
airbnb$Business  <- factor(airbnb$Business )
airbnb$Business  <- factor(airbnb$Business )

str(airbnb)
```

```{r}
library(corrgram)
nums <- unlist(lapply(airbnb, is.numeric), use.names = FALSE)  
air_num <- airbnb[ , nums]



corrgram(airbnb,
         order = TRUE,              # If TRUE, PCA-based re-ordering
         upper.panel = panel.pie,   # Panel function above diagonal
         lower.panel = panel.shade, # Panel function below diagonal
         text.panel = panel.txt,    # Panel function of the diagonal
         main = "Correlogram") 
```

### Feature selection

Price has higher correlation with attraction.index, and restaruant index, and bedroom. These thee using polynomial term, and others are linear term.

Discrete variables: bedroom, person capacity... polynomial may not suitble.

### Training, testing, validation spliting

```{r}
set.seed(51215)

## generate 1-3 as the group of training, testing, and validation set
idx <- sample(seq(1, 3), size = nrow(airbnb), replace = TRUE, prob = c(.75, .15, .15))
train <- airbnb[idx == 1,]
dev <- airbnb[idx == 2,]
test <- airbnb[idx == 3,]

dim(train);dim(dev);dim(test)
```

```{r}
## save the datasets
# write.csv(train, "../data/train.csv")
# write.csv(dev, "../data/dev.csv")
# write.csv(test, "../data/test.csv")
```

Remove duplicate column,Shared.Room and Private.Room (redundant, Room.Type has three levels including these two)

```{r}
airbnb <- airbnb[,!names(airbnb) %in% c("Shared.Room","Private.Room")]
set.seed(51215)

## generate 1-3 as the group of training, testing, and validation set
idx <- sample(seq(1, 3), size = nrow(airbnb), replace = TRUE, prob = c(.75, .15, .15))
train <- airbnb[idx == 1,]
dev <- airbnb[idx == 2,]
test <- airbnb[idx == 3,]

dim(train);dim(dev);dim(test)
```

### Examine relationship between y and x

Price is a highly skewed distribution. Log price has better distribution, thus, log price is used for modeling. Continues features are also logged. 

```{r}
hist(log(air_num$Price))
```


```{r warning=FALSE,message=FALSE}
idx <- sample(nrow(air_num), 500, replace = F)

### log x
for (i in c(4,6,7,8,9,10,11)){
    y = log(air_num[["Price"]]+1)[idx]
    x = log(air_num[[i]]+1)[idx]
    order_x <- order(x)
    y <- y[order_x]
    x <- x[order_x]
    lo <- loess(y~x)
    plot(x = x,y = y,xlab=paste0("log(",colnames(air_num)[i],")"), ylab="log(Price)")
    lines(x = x, y = predict(lo), col='red', lwd=2)
}


### original x
for (i in c(1,2,3,5)){
    y = log(air_num[["Price"]]+1)[idx]
    x = air_num[[i]][idx]
    order_x <- order(x)
    y <- y[order_x]
    x <- x[order_x]
    lo <- loess(y~x)
    plot(x = x,y = y,xlab=colnames(air_num)[i], ylab="log(Price)")
    lines(x = x, y = predict(lo), col='red', lwd=2)
}
```

### Polynomial model with lasso regularization, using CV tuning lambda

Polynomial only works on numeric features. Based on the distribution plots above, 
<ul>

<li>log Guest.Satisfaction can be a linear</li>
<li>log City.Center..km. can be a poly 3</li>
<li>log Metro.Distance..km. can be a poly 3</li>
<li>log Normalised.Attraction.Index can be a poly 4</li>
<li>log Normalised.Restraunt.Index can be a poly 3</li>
<li>Person.Capacity  can be a poly 2 or a linear</li>
<li>Cleanliness.Rating can be a poly 2 or a linear</li>
<li>Bedrooms can be a poly 2</li>

</ul>

Thus, the model is constructed as below.

```{r}
model_nonlinear <-
    lm(
        log(Price) ~ poly(Bedrooms, 2) + 
            poly(Cleanliness.Rating) + 
            Person.Capacity +
            poly(log(Normalised.Restraunt.Index), 3)+
            poly(log(Normalised.Attraction.Index), 4) + poly(log(`Metro.Distance..km.`), 3) +
            poly(log(`City.Center..km.`, 3)) + 
            log(Guest.Satisfaction) +
            City + Day + Superhost, data=train)

summary(model_nonlinear)

```

With lasso regularization
```{r}

## model input
cv_lasso_x <- matrix(poly(df$x,12), ncol = 12, byrow = F)
cv_lasso_y <- as.vector(df$y)

## model
cv_lasso <- cv.glmnet(y = cv_ridge_y, 
                      x = cv_ridge_x,
                      alpha = 1, # 1 is lasso regression
                      standardize = TRUE, 
                      nfolds = 10,
                      thresh = 1e-12)

## prediction
pred <- predict(cv_lasso, s = cv_lasso$lambda.1se, newx = cv_lasso_x)

##1se
cv_lasso$lambda.1se
##rms
(rms_cv_10 <- cv_lasso$cvm[which(cv_lasso$lambda==cv_lasso$lambda.1se)])


## plot
df$yhat_ridge <- as.vector(pred)
p <- df %>%
  ggplot() +
  geom_point(aes(x = x, y = y, color = "Observed"), size = 0.5) + 
  geom_line(aes(x = x, y = yhat_ridge, color = "Predicted"), size = 2, 
            linetype = "dashed") + 
  scale_color_manual(name = "Legend", 
                     values = c("Predicted" = "red", "Observed" = "black")) +
  ggtitle(paste("Polynomial model with lasso regression of degrees of",k)) +
  xlab("Time") + ylab("FTSE")

p
```

For regularization, I choose Lasso regression with full features. In this way, some of the feature will have coefficient of zero, making the model simplier. 
