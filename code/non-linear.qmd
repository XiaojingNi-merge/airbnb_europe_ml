---
title: "Non-linear regression on AirBnb dataset"
author: "Xiaojing Ni Group 15"
format:
    html:
        embed-resources: true
        theme: default
        code-copy: true
        code-line-numbers: true
        number-sections: true
        toc: true
---

```{r results='hide', warning=FALSE, message=FALSE}
library(caret)
library(dplyr)
library(tidyr)
library(rpart)
library(glmnet)
library(rattle)
library(ggplot2)
library(stringr)
library(reshape2)
library(tidyverse)
library(visNetwork)



```

```{r}
## read data

airbnb <- read.csv("../data/Aemf1.csv")
str(airbnb)
```

```{r}
airbnb$City <- factor(airbnb$City)
airbnb$Day <- factor(airbnb$Day)
airbnb$Room.Type <- factor(airbnb$Room.Type)
airbnb$Shared.Room <- factor(airbnb$Shared.Room)
airbnb$Private.Room  <- factor(airbnb$Private.Room )
airbnb$Superhost  <- factor(airbnb$Superhost )
airbnb$Multiple.Rooms  <- factor(airbnb$Multiple.Rooms )
airbnb$Business  <- factor(airbnb$Business )
airbnb$Business  <- factor(airbnb$Business )

str(airbnb)
```

```{r}
library(corrgram)
nums <- unlist(lapply(airbnb, is.numeric), use.names = FALSE)  
air_num <- airbnb[ , nums]



corrgram(airbnb,
         order = TRUE,              # If TRUE, PCA-based re-ordering
         upper.panel = panel.pie,   # Panel function above diagonal
         lower.panel = panel.shade, # Panel function below diagonal
         text.panel = panel.txt,    # Panel function of the diagonal
         main = "Correlogram") 
```

### Feature selection

Price has higher correlation with attraction.index, and restaruant index, and bedroom. These thee using polynomial term, and others are linear term.

Discrete variables: bedroom, person capacity... polynomial may not suitble.

### Training, testing, validation spliting

```{r}
set.seed(51215)

## generate 1-3 as the group of training, testing, and validation set
idx <- sample(seq(1, 3), size = nrow(airbnb), replace = TRUE, prob = c(.75, .15, .15))
train <- airbnb[idx == 1,]
dev <- airbnb[idx == 2,]
test <- airbnb[idx == 3,]

dim(train);dim(dev);dim(test)
```

```{r}
## save the datasets
# write.csv(train, "../data/train.csv")
# write.csv(dev, "../data/dev.csv")
# write.csv(test, "../data/test.csv")
```

Remove duplicate column,Shared.Room and Private.Room (redundant, Room.Type has three levels including these two)

```{r}
airbnb <- airbnb[,!names(airbnb) %in% c("Shared.Room","Private.Room")]
set.seed(51215)

## generate 1-3 as the group of training, testing, and validation set
idx <- sample(seq(1, 3), size = nrow(airbnb), replace = TRUE, prob = c(.75, .15, .15))
train <- airbnb[idx == 1,]
dev <- airbnb[idx == 2,]
test <- airbnb[idx == 3,]

dim(train);dim(dev);dim(test)
```

### Examine relationship between y and x

Price is a highly skewed distribution. Log price has better distribution, thus, log price is used for modeling. Continues features are also logged. 

```{r}
hist(log(air_num$Price))
```


```{r warning=FALSE,message=FALSE}
idx <- sample(nrow(air_num), 500, replace = F)

### log x
for (i in c(4,6,7,8,9,10,11)){
    y = log(air_num[["Price"]]+1)[idx]
    x = log(air_num[[i]]+1)[idx]
    order_x <- order(x)
    y <- y[order_x]
    x <- x[order_x]
    lo <- loess(y~x)
    plot(x = x,y = y,xlab=paste0("log(",colnames(air_num)[i],")"), ylab="log(Price)")
    lines(x = x, y = predict(lo), col='red', lwd=2)
}


### original x
for (i in c(1,2,3,5)){
    y = log(air_num[["Price"]]+1)[idx]
    x = air_num[[i]][idx]
    order_x <- order(x)
    y <- y[order_x]
    x <- x[order_x]
    lo <- loess(y~x)
    plot(x = x,y = y,xlab=colnames(air_num)[i], ylab="log(Price)")
    lines(x = x, y = predict(lo), col='red', lwd=2)
}
```

### Polynomial model with lasso regularization, using CV tuning lambda

Polynomial only works on numeric features. Based on the distribution plots above, 
<ul>

<li>log Guest.Satisfaction can be a linear</li>
<li>log City.Center..km. can be a poly 3</li>
<li>log Metro.Distance..km. can be a poly 3</li>
<li>log Normalised.Attraction.Index can be a poly 4</li>
<li>log Normalised.Restraunt.Index can be a poly 3</li>
<li>Person.Capacity  can be a poly 2 or a linear</li>
<li>Cleanliness.Rating can be a poly 2 or a linear</li>
<li>Bedrooms can be a poly 2</li>

</ul>

Thus, the model is constructed as below.

```{r}
model_nonlinear <-
    lm(
        log(Price) ~ poly(Bedrooms, 2) + 
            poly(Cleanliness.Rating) + 
            Person.Capacity +
            poly(log(Normalised.Restraunt.Index), 3)+
            poly(log(Normalised.Attraction.Index), 4) + poly(log(`Metro.Distance..km.`), 3) +
            poly(log(`City.Center..km.`), 3) + 
            log(Guest.Satisfaction) +
            City + Day + Superhost, data=train)

summary(model_nonlinear)

```

With lasso regularization
```{r}
# empty list
train_list <- vector(mode='list', length=9)

train_list[[1]] <- matrix(poly(train$Bedrooms, 2), ncol = 2, byrow = F)
colnames(train_list[[1]]) <- c("Bedrooms1", "Bedrooms2")

train_list[[2]] <- matrix(train$Cleanliness.Rating, ncol = 1)
colnames(train_list[[2]]) <- c("Cleanliness.Rating")

train_list[[3]] <- matrix(train$Person.Capacity, ncol = 1)
colnames(train_list[[3]]) <- c("Person.Capacity")

train_list[[4]] <- matrix(poly(log(train$Normalised.Restraunt.Index), 3),
                          ncol = 3, byrow = F)
colnames(train_list[[4]]) <- paste("log(Normalised.Restraunt.Index)", 
                                   seq.int(3), sep = "")

train_list[[5]] <- matrix(poly(log(train$Normalised.Attraction.Index), 4),
                          ncol = 4, byrow = F)
colnames(train_list[[5]]) <- paste("log(Normalised.Attraction.Index)", 
                                   seq.int(4), sep = "")

train_list[[6]] <- matrix(poly(log(train$`Metro.Distance..km.`), 3),
                          ncol = 3, byrow = F)
colnames(train_list[[6]]) <- paste("log(Metro.Distance..km.)", 
                                   seq.int(3), sep = "")

train_list[[7]] <- matrix(poly(log(train$`City.Center..km.`), 3),
                          ncol = 3, byrow = F)
colnames(train_list[[7]]) <- paste("log(City.Center..km.)", 
                                   seq.int(3), sep = "")

train_list[[8]] <- matrix(log(train$Guest.Satisfaction), ncol = 1)
colnames(train_list[[8]]) <- c("log(Guest.Satisfaction)")

train_list[[9]] <- model.matrix(~ Day + City + Superhost, data = train)
train_list[[9]] <- train_list[[9]][,-1] ## remove Intercept

## sanity check
lapply(train_list, dim)

cv_lasso_x <- do.call("cbind", train_list)
```

```{r}

cv_lasso_y <- as.vector(log(train$Price))

## model, 10 fold cv on alpha
cv_lasso <- cv.glmnet(y = cv_lasso_y, 
                      x = cv_lasso_x,
                      alpha = 1, # 1 is lasso regression
                      standardize = TRUE, 
                      nfolds = 10,
                      thresh = 1e-12)

## prediction
pred <- predict(cv_lasso, s = cv_lasso$lambda.1se, newx = cv_lasso_x)

##1se, best (note: cv best point is not the minium)
cv_lasso$lambda.1se
##rms
(rms_cv_10 <- cv_lasso$cvm[which(cv_lasso$lambda==cv_lasso$lambda.1se)])
## CV sd
(mean(cv_lasso$cvsd))

## plot
train$yhat_lasso <- as.vector(pred)
train$logy <- log(train$Price)
ggplot(train, aes(x=logy, y= yhat_lasso)) +
  geom_point() +
  geom_abline(intercept=0, slope=1) +
  labs(x='Observed log prices', y='Predicted log prices', title='Predicted vs. Observed Values')
```
### Training MSE for log price
```{r}
mean((train$logy - train$yhat_lasso)^2)
```
### cv rms
```{r}
##rms
(rms_cv_10 <- cv_lasso$cvm[which(cv_lasso$lambda==cv_lasso$lambda.1se)])
```
### CV sd
```{r}
## CV sd
(mean(cv_lasso$cvsd))
```

For regularization, I choose Lasso regression with full features. In this way, some of the feature will have coefficient of zero, making the model simplier. 

### Test error
```{r}
# empty list
test_list <- vector(mode='list', length=9)

test_list[[1]] <- matrix(poly(test$Bedrooms, 2), ncol = 2, byrow = F)
colnames(test_list[[1]]) <- c("Bedrooms1", "Bedrooms2")

test_list[[2]] <- matrix(test$Cleanliness.Rating, ncol = 1)
colnames(test_list[[2]]) <- c("Cleanliness.Rating")

test_list[[3]] <- matrix(test$Person.Capacity, ncol = 1)
colnames(test_list[[3]]) <- c("Person.Capacity")

test_list[[4]] <- matrix(poly(log(test$Normalised.Restraunt.Index), 3),
                          ncol = 3, byrow = F)
colnames(test_list[[4]]) <- paste("log(Normalised.Restraunt.Index)", 
                                   seq.int(3), sep = "")

test_list[[5]] <- matrix(poly(log(test$Normalised.Attraction.Index), 4),
                          ncol = 4, byrow = F)
colnames(test_list[[5]]) <- paste("log(Normalised.Attraction.Index)", 
                                   seq.int(4), sep = "")

test_list[[6]] <- matrix(poly(log(test$`Metro.Distance..km.`), 3),
                          ncol = 3, byrow = F)
colnames(test_list[[6]]) <- paste("log(Metro.Distance..km.)", 
                                   seq.int(3), sep = "")

test_list[[7]] <- matrix(poly(log(test$`City.Center..km.`), 3),
                          ncol = 3, byrow = F)
colnames(test_list[[7]]) <- paste("log(City.Center..km.)", 
                                   seq.int(3), sep = "")

test_list[[8]] <- matrix(log(test$Guest.Satisfaction), ncol = 1)
colnames(test_list[[8]]) <- c("log(Guest.Satisfaction)")

test_list[[9]] <- model.matrix(~ Day + City + Superhost, data = test)
test_list[[9]] <- test_list[[9]][,-1] ## remove Intercept

## sanity check
lapply(test_list, dim)

test_x <- do.call("cbind", test_list)
```

```{r}

cv_lasso_y <- as.vector(log(test$Price))

## prediction
pred_test <- predict(cv_lasso, s = cv_lasso$lambda.1se, newx = test_x)

## plot
test$yhat_lasso <- as.vector(pred_test)
test$logy <- log(test$Price)
ggplot(test, aes(x=logy, y= yhat_lasso)) +
  geom_point() +
  geom_abline(intercept=0, slope=1) +
  labs(x='Observed log prices', y='Predicted log prices', title='Predicted vs. Observed Values for test set')

## MSE

mean((test$logy - test$yhat_lasso)^2)

```
